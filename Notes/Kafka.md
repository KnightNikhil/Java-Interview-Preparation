‚úÖ 1. Kafka Basics
* What is Kafka and why it‚Äôs used (high-throughput, distributed, fault-tolerant messaging)
* Kafka vs JMS/RabbitMQ
* Core concepts:
* Producer, Consumer
* Broker, Topic, Partition
* Offset, Consumer Group
* Leader/Follower

‚∏ª

‚úÖ 2. Kafka Architecture
* Cluster and broker setup
* Topics and partitions
* Leader election and replication
* Zookeeper role (Kafka 2.x) and KRaft mode (Kafka 3.x+)
* High availability and fault tolerance

‚∏ª

‚úÖ 3. Kafka Producer API (Java)
* Creating Kafka producers
* ProducerRecord, KafkaProducer
* Acknowledgment modes (acks = 0, 1, all)
* Key serialization and partitioning logic
* Error handling and retries
* Idempotent producers

‚∏ª

‚úÖ 4. Kafka Consumer API (Java)
* Creating Kafka consumers
* KafkaConsumer, poll(), subscribe()
* Consumer groups and load balancing
* Manual vs auto offset commit
* Rebalancing and partition assignment strategies
* Consuming from a specific offset or timestamp
* Handling duplicates and idempotent processing

‚∏ª

‚úÖ 5. Message Serialization
* Kafka serializers/deserializers
* String, ByteArray
* JSON, Avro, Protobuf (Schema Registry)
* Custom serializers and deserializers

‚∏ª

‚úÖ 6. Kafka with Spring Boot
* Using Spring Kafka (spring-kafka)
* @KafkaListener, KafkaTemplate
* Listener container config (ConcurrentKafkaListenerContainerFactory)
* Error handling and retry logic
* Dead Letter Topics (DLTs)
* Producer and Consumer configuration

‚∏ª

‚úÖ 7. Kafka Streams API
* Stream processing directly on Kafka topics
* Stateless vs stateful transformations
* map, filter, join, windowed operations
* KStream, KTable, GlobalKTable
* Use cases vs Kafka Connect / external stream processors

‚∏ª

‚úÖ 8. Kafka Connect
* Moving data into/from Kafka using connectors (source/sink)
* JDBC, File, ElasticSearch, S3 connectors
* Custom connector creation
* Connect cluster configuration

‚∏ª

‚úÖ 9. Kafka Security
* SSL encryption (TLS)
* SASL Authentication (PLAIN, SCRAM)
* ACLs (Access Control Lists)
* Securing with tools like Kafka-ACLs, Kerberos (optional)

‚∏ª

‚úÖ 10. Monitoring and Metrics
* Kafka JMX metrics
* Tools: Prometheus + Grafana, Confluent Control Center
* Consumer lag monitoring
* Dead-letter queues

‚∏ª

‚úÖ 11. Error Handling & Retries
* Deserialization errors
* Retry strategies: fixed backoff, exponential backoff
* Dead-letter topics (DLTs)
* Poison pill messages

‚∏ª

‚úÖ 12. Performance Tuning
* Batching and compression
* Linger.ms, batch.size, buffer.memory
* fetch.min.bytes, fetch.max.wait.ms
* Parallelism in consumers
* Partitioning strategies

‚∏ª

‚úÖ 13. High Availability & Scalability
* Partitioning strategy and balancing
* Replication factor and ISR (In-Sync Replicas)
* Avoiding data loss (min.insync.replicas, acks=all)
* Multi-cluster architecture (MirrorMaker)

‚∏ª

‚úÖ 14. Schema Registry (with Avro/Protobuf)
* Storing message schemas centrally
* Evolving schemas with compatibility rules
* Strong typing in Kafka messages

‚∏ª

‚úÖ 15. Real-World Design Patterns
* Event Sourcing with Kafka
* CQRS using Kafka streams
* Saga Pattern with Kafka (choreography-based coordination)
* Idempotent consumers (exactly-once processing)
* Outbox pattern with DB-Kafka sync

‚∏ª

‚úÖ 16. Testing Kafka
* Embedded Kafka for unit/integration tests
* Kafka TestContainers
* Simulating partitions, lag, retries

----

# ‚úÖ Kafka Basics

## What is Kafka?
Apache Kafka is a distributed event streaming platform used for building real-time data pipelines and streaming applications. It is highly scalable, fault-tolerant, and designed for high-throughput data ingestion and processing.

### Why Kafka is Used?
- **High Throughput**: Capable of handling millions of messages per second.
- **Distributed**: Scales horizontally and handles data replication.
- **Fault-Tolerant**: Automatically recovers from failures.
- **Durable**: Messages are stored on disk and replicated across brokers.
- **Real-Time Processing**: Ideal for time-sensitive data delivery and streaming.

---

## Kafka vs JMS / RabbitMQ

| Feature                | Kafka                          | JMS / RabbitMQ                |
|------------------------|--------------------------------|-------------------------------|
| Messaging Model        | Log-based publish-subscribe    | Message queue (push model)    |
| Performance            | High throughput & low latency  | Lower throughput              |
| Persistence            | Persistent with disk storage   | Mostly in-memory              |
| Scalability            | Highly scalable horizontally   | Limited scalability           |
| Replayability          | Yes (via offset)               | No replay once consumed       |
| Consumer Model         | Pull-based                     | Push-based                    |

---

## Core Concepts

### Producer
A **producer** sends messages (records) to Kafka **topics**.

```java
ProducerRecord<String, String> record = new ProducerRecord<>("topic-name", "key", "value");
producer.send(record);
```

### Consumer
A **consumer** reads messages from topics. Consumers can belong to **consumer groups**.

```java
consumer.subscribe(Collections.singletonList("topic-name"));
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
```

### Broker
A **Kafka broker** is a server that stores data and serves clients (producers/consumers). Kafka clusters consist of multiple brokers.

### Topic
A **topic** is a named stream of records. Producers write to topics, and consumers read from them.

### Partition
Each topic is split into **partitions** to allow for parallelism and scalability.

### Offset
Each record within a partition has a unique **offset** that identifies it. Consumers use offsets to keep track of what has been read.

### Consumer Group
A **consumer group** is a set of consumers that work together to consume data from a topic. Kafka ensures that each partition is consumed by only one consumer in the group.

### Leader/Follower
- Each partition has a **leader** (handling reads/writes) and zero or more **followers** (replicas for redundancy).
- **Followers** replicate the leader‚Äôs data and take over if the leader fails.

---

## Follow-Up Interview Questions

### Q1: How does Kafka ensure message durability?
**A:** Messages are written to disk and replicated across multiple brokers using a configurable replication factor.

### Q2: What is the difference between a Kafka topic and a partition?
**A:** A topic is a logical stream of data, and a partition is a subset of the topic that allows Kafka to parallelize processing.

### Q3: What happens when a consumer fails?
**A:** Kafka will reassign the partitions that were assigned to the failed consumer to other active consumers in the group.

### Q4: How does Kafka handle backpressure?
**A:** Kafka allows consumers to pull messages at their own pace and provides offset management to avoid overload.

### Q5: Can messages be reprocessed?
**A:** Yes, by resetting the consumer offset to an earlier position or to the beginning of the partition.

---


### ‚úÖ 2. Kafka Architecture

---

#### üîπ Cluster and Broker Setup

- **Kafka Cluster**: A Kafka cluster consists of multiple **brokers**, each running on a separate machine (or container).
- **Broker**: A Kafka broker is a single Kafka server that handles read/write requests from clients (producers and consumers), manages topic partitions, and persists data.

```
Kafka Cluster
 ‚îú‚îÄ‚îÄ Broker 1
 ‚îú‚îÄ‚îÄ Broker 2
 ‚îî‚îÄ‚îÄ Broker 3
```

- Each broker has a unique ID and is responsible for one or more **partitions** of a topic.
- Kafka scales horizontally: you add more brokers to handle more load.

---

#### üîπ Topics and Partitions

- **Topic**: A category to which records are sent by producers.
- **Partition**: Topics are split into partitions to allow parallel processing and scalability.

```
Topic: orders
 ‚îú‚îÄ‚îÄ Partition 0 (Broker 1)
 ‚îú‚îÄ‚îÄ Partition 1 (Broker 2)
 ‚îî‚îÄ‚îÄ Partition 2 (Broker 3)
```

- Each message within a partition has an **offset**, a unique sequence number.
- **Partitioning** allows:
    - Horizontal scaling
    - Load distribution
    - Message ordering per partition

---

#### üîπ Leader Election and Replication

- **Replication** ensures **fault tolerance**.
- Each partition has:
    - One **leader**: handles all read/write for that partition.
    - Zero or more **followers**: replicate the leader's data.

```
Partition 0:
 - Leader: Broker 1
 - Follower: Broker 2
```

- If a broker with the leader partition fails, a follower is elected as the new leader by **Apache ZooKeeper** (or KRaft in Kafka 3+).

---

#### üîπ Zookeeper Role (Kafka 2.x)

- Kafka uses **ZooKeeper** for:
    - Cluster coordination
    - Leader election
    - Metadata management

- Kafka nodes register with ZooKeeper, and ZooKeeper tracks broker health and controls topic-partition assignments.

---

#### üîπ KRaft Mode (Kafka 3.x+)

- Kafka now supports **KRaft (Kafka Raft Metadata mode)** to eliminate ZooKeeper.
- Features of KRaft:
    - Built-in consensus mechanism (Raft protocol)
    - Simplified deployment (no ZooKeeper)
    - Better performance and scalability

```
Kafka KRaft Mode
 ‚îú‚îÄ‚îÄ Controller quorum (Raft)
 ‚îî‚îÄ‚îÄ Brokers
```

---

#### üîπ High Availability and Fault Tolerance

Kafka ensures high availability by:
- **Replicating partitions** across multiple brokers
- Using **acks** to control delivery guarantees:
    - `acks=0`: no guarantee
    - `acks=1`: leader acknowledgment
    - `acks=all`: leader + all ISR (in-sync replicas) acknowledge

- **Consumer group rebalance** on failure
- **Retention policies** ensure data durability

---

### ‚úÖ Sample Interview Questions and Answers

---

**Q1. What happens when the leader of a partition goes down?**  
**A:** Kafka triggers a **leader election** process via ZooKeeper (or KRaft), and one of the in-sync followers becomes the new leader.

---

**Q2. How does Kafka ensure fault tolerance?**  
**A:** By **replicating partitions**, using **acks**, and **leader-follower** election. Consumers and producers can resume from the last committed **offset** in case of failure.

---

**Q3. What is the role of ZooKeeper in Kafka?**  
**A:** In Kafka 2.x, ZooKeeper manages broker metadata, leader elections, and configuration. From Kafka 3.x onward, ZooKeeper is replaced by **KRaft** for better performance.

---

**Q4. How does Kafka achieve scalability?**  
**A:** Kafka uses **partitioning** to split topics into multiple parts that can be spread across brokers, allowing parallel processing and high throughput.

---

# Kafka Producer API (Java)

## ‚úÖ Overview
Kafka Producers are used to publish data (records/messages) to Kafka topics. They push data to Kafka brokers based on the topic and optional key, which determines the partition.

---

## ‚úÖ 1. Creating Kafka Producers

### üß± Components:
- `ProducerRecord<K, V>`: Represents a record with topic, key, value, partition, and timestamp.
- `KafkaProducer<K, V>`: Main class to send records.

### üí° Example:

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

ProducerRecord<String, String> record = new ProducerRecord<>("my-topic", "key1", "value1");
producer.send(record);
producer.close();
```

---

## ‚úÖ 2. Acknowledgment Modes (`acks`)

| acks Value | Description |
|------------|-------------|
| 0          | No acknowledgment; fire and forget |
| 1          | Leader acknowledges receipt |
| all (or -1)| All in-sync replicas acknowledge |

**Set via:**
```java
props.put("acks", "all");
```

---

## ‚úÖ 3. Key Serialization & Partitioning

- Kafka uses key's hash to determine partition.
- If no key is provided, Kafka uses round-robin.

```java
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
```

You can also implement a **CustomPartitioner**.

---

## ‚úÖ 4. Error Handling and Retries

### üìå Configurations:

- `retries`: Number of retry attempts (default: 0).
- `retry.backoff.ms`: Wait time between retries.

```java
props.put("retries", 3);
props.put("retry.backoff.ms", 1000);
```

### üìå Synchronous Send with Exception Handling:

```java
try {
    producer.send(record).get(); // Blocking
} catch (Exception e) {
    e.printStackTrace();
}
```

---

## ‚úÖ 5. Idempotent Producer

Ensures **exactly-once** semantics in Kafka 0.11+.

```java
props.put("enable.idempotence", "true");
```

Benefits:
- Prevents duplicate messages on retries
- Automatically sets safe configurations (`acks=all`, `retries > 0`, `max.in.flight.requests.per.connection=5`)

---

## ‚úÖ Summary Table

| Feature           | Key Config / Class                                 |
|-------------------|-----------------------------------------------------|
| Basic Producer    | `KafkaProducer`, `ProducerRecord`                  |
| Acks              | `acks = 0 / 1 / all`                                |
| Retry             | `retries`, `retry.backoff.ms`                      |
| Serialization     | `key.serializer`, `value.serializer`               |
| Idempotence       | `enable.idempotence = true`                        |
| Partitioning      | `CustomPartitioner` (optional)                     |

---

# ‚úÖ Kafka Consumer API (Java)

## üìå Overview

The **Kafka Consumer API** allows applications to read streams of data from Kafka topics efficiently and with fault tolerance. Consumers are typically part of a **Consumer Group**, which allows Kafka to scale horizontally.

---

## üîπ Creating Kafka Consumers

To create a Kafka consumer in Java:

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("group.id", "my-consumer-group");
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
consumer.subscribe(Collections.singletonList("my-topic"));
```

---

## üîπ poll() and subscribe()

- `poll(Duration timeout)`: Fetches data from Kafka.
- `subscribe(List<String>)`: Subscribes the consumer to one or more topics.

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("Consumed: key=%s, value=%s, offset=%d%n",
                          record.key(), record.value(), record.offset());
    }
}
```

---

## üîπ Consumer Groups and Load Balancing

- Kafka consumers in the **same group** share load.
- Each partition is consumed by only **one consumer** in the group.
- Multiple groups = multiple independent reads of data.

---

## üîπ Offset Commit Strategies

### üî∏ Automatic Offset Commit
Set `enable.auto.commit=true` (default). Kafka will periodically commit offsets.

**Pros:**
- Easy to use.

**Cons:**
- Risk of **duplicate** processing on crashes.

---

### üî∏ Manual Offset Commit

```java
consumer.commitSync(); // or commitAsync()
```

Control **when** offsets are committed. Ensures processing is complete before acknowledging.

---

## üîπ Rebalancing and Partition Assignment Strategies

When consumers join or leave the group, Kafka **rebalances** partitions.

Common strategies:
- `RangeAssignor` (default): continuous chunk of partitions.
- `RoundRobinAssignor`: assigns partitions evenly across consumers.

```java
props.put("partition.assignment.strategy",
          "org.apache.kafka.clients.consumer.RoundRobinAssignor");
```

---

## üîπ Consuming from a Specific Offset or Timestamp

```java
TopicPartition partition = new TopicPartition("my-topic", 0);
consumer.assign(Collections.singletonList(partition));
consumer.seek(partition, 10); // Start from offset 10
```

For timestamp:

```java
Map<TopicPartition, Long> timestamps = new HashMap<>();
timestamps.put(partition, System.currentTimeMillis() - 3600000);
Map<TopicPartition, OffsetAndTimestamp> offsets = consumer.offsetsForTimes(timestamps);
consumer.seek(partition, offsets.get(partition).offset());
```

---

## üîπ Handling Duplicates and Idempotent Processing

Kafka **does not guarantee exactly-once delivery** on the consumer side. To ensure idempotency:

- Track processed message keys or offsets.
- Use external storage or deduplication logic.
- Combine with transactional producer for true exactly-once.

---

## ‚úÖ Interview Questions & Answers

### Q1: What is a Kafka consumer group?

**A:** A group of consumers that coordinate to consume partitions of a topic without overlap. It enables horizontal scaling of message processing.

---

### Q2: How does Kafka handle load balancing?

**A:** Kafka uses **consumer groups** to assign partitions such that each partition is consumed by only one consumer in a group.

---

### Q3: What are rebalancing events?

**A:** When a consumer joins or leaves, Kafka reassigns partitions among the active consumers. This is called rebalancing.

---

### Q4: When should you use manual offset commit?

**A:** Use manual offset commit when you want to ensure that processing logic is successfully completed before acknowledging the message.

---

### Q5: Can a Kafka consumer read from a specific timestamp?

**A:** Yes, using `offsetsForTimes()` to get the offset for a given timestamp and then `seek()`.

---

# Kafka with Spring Boot

## ‚úÖ Using Spring Kafka (`spring-kafka`)
Spring Kafka provides a simple abstraction for integrating Apache Kafka with Spring applications. The main dependency is:

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

---

## ‚úÖ @KafkaListener, KafkaTemplate

### KafkaTemplate
Used to send messages to Kafka topics.

```java
@Autowired
private KafkaTemplate<String, String> kafkaTemplate;

public void sendMessage(String topic, String message) {
    kafkaTemplate.send(topic, message);
}
```

### @KafkaListener
Used to consume messages.

```java
@KafkaListener(topics = "my-topic", groupId = "group_id")
public void listen(String message) {
    System.out.println("Received: " + message);
}
```

---

## ‚úÖ Listener Container Config (`ConcurrentKafkaListenerContainerFactory`)

Used to configure how consumers behave.

```java
@Bean
public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory<String, String> factory =
            new ConcurrentKafkaListenerContainerFactory<>();
    factory.setConsumerFactory(consumerFactory());
    factory.setConcurrency(3); // for parallel processing
    return factory;
}
```

---

## ‚úÖ Error Handling and Retry Logic

You can handle errors by customizing the error handler.

```java
factory.setErrorHandler((thrownException, data) -> {
    System.out.println("Error in process with Exception {} and the record is {}" + thrownException + data);
});
```

Spring Kafka also supports retry logic through `RetryTemplate` or Dead Letter Topics.

---

## ‚úÖ Dead Letter Topics (DLTs)

DLTs help handle message processing failures.

```yaml
spring:
  kafka:
    listener:
      ack-mode: manual
      retry:
        enabled: true
      error-handler:
        dead-letter-publish: true
```

Example configuration:
```java
@Bean
public DeadLetterPublishingRecoverer publisher(KafkaTemplate<Object, Object> template) {
    return new DeadLetterPublishingRecoverer(template);
}
```

---

## ‚úÖ Producer and Consumer Configuration

### Producer config:
```java
@Bean
public Map<String, Object> producerConfigs() {
    Map<String, Object> props = new HashMap<>();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
    return props;
}
```

### Consumer config:
```java
@Bean
public Map<String, Object> consumerConfigs() {
    Map<String, Object> props = new HashMap<>();
    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    props.put(ConsumerConfig.GROUP_ID_CONFIG, "group_id");
    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    return props;
}
```

---

## ‚úÖ Follow-Up Interview Questions

1. **How does Spring Kafka handle message retries?**
    - Via `RetryTemplate` or Dead Letter Topics.
2. **What happens if message processing fails in a Kafka listener?**
    - It can be retried or forwarded to a DLT.
3. **How do you configure concurrent consumers in Spring Kafka?**
    - Using `setConcurrency()` in `ConcurrentKafkaListenerContainerFactory`.
4. **Difference between `@KafkaListener` and using KafkaConsumer directly?**
    - `@KafkaListener` is simpler and uses a listener container internally.

---

# Kafka Streams API

Kafka Streams is a client library for building applications and microservices that process and transform data stored in Kafka. It combines the simplicity of writing and deploying standard Java applications with the benefits of Kafka's server-side cluster technology.

---

## ‚úÖ Stream Processing on Kafka Topics

Kafka Streams allows direct processing of records in Kafka topics without requiring separate processing clusters (like Flink or Spark). It can read, process, and write back to Kafka‚Äîall inside a single application.

---

## ‚úÖ Stateless vs Stateful Transformations

- **Stateless**: Operations do not depend on previous records.
    - Examples: `map`, `filter`, `flatMap`

- **Stateful**: Require maintaining state or relationships across records.
    - Examples: `groupByKey`, `count`, `aggregate`, `join`, `windowed operations`

---

## ‚úÖ Core Transformations

### üîπ map, filter, flatMap
```java
stream.filter((key, value) -> value.contains("error"))
      .mapValues(value -> value.toUpperCase());
```

### üîπ join
```java
KStream<String, Order> orders = builder.stream("orders");
KTable<String, Customer> customers = builder.table("customers");

KStream<String, EnrichedOrder> enriched = orders.join(
    customers,
    (order, customer) -> new EnrichedOrder(order, customer)
);
```

### üîπ Windowed operations
```java
stream.groupByKey()
      .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
      .count();
```

---

## ‚úÖ KStream, KTable, GlobalKTable

- **KStream**: Represents an unbounded stream of records.
- **KTable**: Represents a changelog stream as a table (latest state per key).
- **GlobalKTable**: Replicates the full table to each application instance.

Use appropriate types depending on the need for stateful joins or lookups.

---

## ‚úÖ Kafka Streams vs Kafka Connect or External Stream Processors

| Feature | Kafka Streams | Kafka Connect | Flink/Spark |
|--------|----------------|----------------|-------------|
| Type | Library (embedded) | Connector framework | External system |
| Use Case | In-app processing | Ingest/export data | Complex/large-scale processing |
| Dependency | Minimal | Runs in Kafka ecosystem | External infra needed |

---

## ‚úÖ Example Use Case

```java
Properties props = new Properties();
props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-app");
props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());

StreamsBuilder builder = new StreamsBuilder();
KStream<String, String> stream = builder.stream("input-topic");

stream.filter((key, value) -> value.contains("important"))
      .mapValues(String::toUpperCase)
      .to("output-topic");

KafkaStreams streams = new KafkaStreams(builder.build(), props);
streams.start();
```

---

## ‚úÖ Follow-up Interview Questions

### Q1. What are the differences between KStream and KTable?
**A:** `KStream` is for real-time data (event streams), while `KTable` holds the latest value for each key (like a database view). Use `KStream` for stateless, `KTable` for stateful operations.

---

### Q2. What is a windowed join in Kafka Streams?
**A:** It limits the join to events that occur within a specified time window (e.g., join user logins with events within 5 minutes). Useful for temporal correlation.

---

### Q3. What happens if a Kafka Streams app crashes?
**A:** Kafka Streams uses changelog topics and internal state stores to recover state after crashes, ensuring exactly-once semantics.

---

### Q4. When would you choose Kafka Streams over Apache Flink?
**A:** Kafka Streams is ideal for lightweight, embedded stream processing in Java apps. Choose Flink for complex distributed processing, non-JVM support, or large-scale analytics.

---

# ‚úÖ 8. Kafka Connect

Kafka Connect is a framework to stream data into and out of Apache Kafka. It comes with pre-built connectors and also allows the creation of custom ones.

---

## üîπ What is Kafka Connect?

Kafka Connect is part of the Apache Kafka project. It‚Äôs designed to simplify the process of integrating Kafka with external systems such as databases, key-value stores, search indexes, and file systems.

- Moves large data sets efficiently
- Reuses connectors instead of writing custom code
- Scales elastically and fault-tolerantly

---

üîπ Why Use Kafka Connect?

- Simplifies integration between Kafka and external systems
- Offers reusable components called connectors
- Supports both source connectors (importing data into Kafka) and sink connectors (exporting data from Kafka)
- Distributed and fault-tolerant

---

üî∏ Kafka Connect Architecture

- Standalone Mode: For development/testing (runs on a single process)
- Distributed Mode: Production-ready, supports scalability and fault tolerance
- Workers: Kafka Connect processes that run connectors
- Tasks: Units of work performed by a connector (can be parallelized)
- Connector Plugin: JAR file containing logic for interacting with an external system

---
## üîπ Connectors

### ‚úÖ Source Connectors
Used to pull data from external systems into Kafka.

**Examples:**
- JDBC Source Connector
- FileStream Source Connector

### ‚úÖ Sink Connectors
Used to push data from Kafka topics to external systems.

**Examples:**
- JDBC Sink Connector
- Elasticsearch Sink Connector
- S3 Sink Connector

---

## üîπ Popular Connectors

| Connector Type | Examples |
|----------------|----------|
| Source         | JDBC, File, MQTT, HTTP, Twitter |
| Sink           | JDBC, S3, Elasticsearch, HDFS   |

---
üî∏ Example: JDBC Source Connector

```json```
{
  "name": "jdbc-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "tasks.max": "1",
    "connection.url": "jdbc:mysql://localhost:3306/mydb",
    "connection.user": "user",
    "connection.password": "password",
    "topic.prefix": "jdbc-",
    "poll.interval.ms": "1000"
  }
}
---

üî∏ Kafka Connect Cluster Configuration

- Bootstrap Servers: Kafka brokers to connect to
- Key Converter / Value Converter:
  - e.g., org.apache.kafka.connect.json.JsonConverter
- Internal Topics:
  - Config storage topic
  - Offset storage topic
  - Status storage topic
- Rest Port: Connects over REST API (default: 8083)

---
## üîπ Custom Connector Creation

To create a custom Kafka connector:

1. Implement `SourceConnector` or `SinkConnector`
2. Implement `SourceTask` or `SinkTask`
3. Package into a JAR
4. Deploy in the Connect cluster

**Basic Structure:**
```java
public class MyCustomSourceConnector extends SourceConnector {
    // implementation logic
}
```

---

## üîπ Connect Cluster Configuration

Kafka Connect can be run in two modes:

### 1. Standalone Mode
- Suitable for development or testing.
- All configuration in local files.

### 2. Distributed Mode
- Production-ready.
- Distributed, scalable, and fault-tolerant.

**Key configuration files:**
- `connect-standalone.properties`
- `connect-distributed.properties`

**REST API support for managing connectors** in distributed mode.

---

## ‚úÖ Summary

Kafka Connect simplifies integration between Kafka and external systems. With built-in scalability, fault tolerance, and the ability to write custom connectors, it‚Äôs an essential tool for building real-time data pipelines.

---

## üéØ Interview Questions

1. **What is Kafka Connect and how is it different from Kafka Streams?**
    - Kafka Connect is for moving data in/out of Kafka. Kafka Streams is for processing data within Kafka.

2. **What‚Äôs the difference between a source and sink connector?**
    - Source pulls data **into** Kafka, sink pushes data **out** to external systems.

3. **How do you implement a custom Kafka Connector?**
    - Extend `SourceConnector` or `SinkConnector`, implement associated Task classes, and package them as a JAR.

4. **What‚Äôs the difference between standalone and distributed mode in Kafka Connect?**
    - Standalone is for dev/testing, distributed is production-grade with REST management.

5. **Can we manage connectors dynamically?**
    - Yes, in distributed mode using Kafka Connect REST API.

---

# ‚úÖ Kafka Security

## üîê 1. SSL Encryption (TLS)

**Purpose**:  
Encrypt data in transit between Kafka clients and brokers.

**How to Enable**:
- Generate SSL certificates for broker and clients.
- Configure Kafka broker:
```properties
ssl.keystore.location=/path/to/kafka.server.keystore.jks  
ssl.keystore.password=secret  
ssl.key.password=secret  
ssl.truststore.location=/path/to/kafka.server.truststore.jks  
ssl.truststore.password=secret  
security.inter.broker.protocol=SSL  
listeners=SSL://localhost:9093  
```

- Configure client:
```properties
security.protocol=SSL  
ssl.truststore.location=/path/to/client.truststore.jks  
ssl.truststore.password=secret  
```

**Interview Qs**:
- *Why is SSL required in Kafka?*  
  To ensure encryption and prevent man-in-the-middle attacks.
- *Can you encrypt only client-broker traffic?*  
  Yes, by selectively enabling SSL on listener ports.

---

## üîê 2. SASL Authentication

**Purpose**:  
Verify client identity (authentication).

**Mechanisms**:
- **PLAIN** (username/password)
- **SCRAM-SHA-256/512** (secure salted hash passwords)
- **GSSAPI (Kerberos)** (enterprise-level)

**Config for Broker** (SCRAM):
```properties
listeners=SASL_SSL://localhost:9094  
security.inter.broker.protocol=SASL_SSL  
sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512  
sasl.enabled.mechanisms=SCRAM-SHA-512  
```

**Client Config**:
```properties
security.protocol=SASL_SSL  
sasl.mechanism=SCRAM-SHA-512  
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required  
username="admin" password="admin-secret";
```

**Interview Qs**:
- *Difference between PLAIN and SCRAM?*  
  SCRAM stores hashed/salted credentials, more secure than PLAIN.
- *How does Kafka store credentials for SCRAM?*  
  In ZooKeeper (Kafka <3.x) or in KRaft metadata (Kafka 3.x+).

---

## üîê 3. ACLs (Access Control Lists)

**Purpose**:  
Authorization ‚Äî control which users can access which Kafka resources.

**Enable ACLs**:
```properties
authorizer.class.name=kafka.security.authorizer.AclAuthorizer  
super.users=User:admin  
allow.everyone.if.no.acl.found=false
```

**Add ACL Example**:
```sh
kafka-acls.sh --authorizer-properties zookeeper.connect=localhost:2181  --add --allow-principal User:clientA --operation Read --topic myTopic
```

**Interview Qs**:
- *How do ACLs work in Kafka?*  
  They specify which principals (users) can perform operations on resources like topics, consumer groups.
- *What happens if no ACL is defined?*  
  Controlled by `allow.everyone.if.no.acl.found` (true allows access, false blocks).

---

## üîê 4. Kerberos (Optional, for enterprise)

**Purpose**:  
Enterprise-grade SSO and authentication with GSSAPI.

**Broker Config**:
```properties
security.inter.broker.protocol=SASL_PLAINTEXT  
sasl.mechanism.inter.broker.protocol=GSSAPI  
```

**Client Config**:
```properties
security.protocol=SASL_PLAINTEXT  
sasl.mechanism=GSSAPI  
sasl.kerberos.service.name=kafka  
```

**Key Concepts**:
- Uses **Ticket Granting Ticket (TGT)** and **Keytab files**
- Integrated with LDAP/Active Directory

**Interview Qs**:
- *When is Kerberos preferable over SCRAM?*  
  In large enterprise environments where SSO and centralized identity management are required.
- *What are challenges with Kerberos in Kafka?*  
  Complex setup, requires Kerberos KDC and proper time sync (NTP).

---

## ‚úÖ Summary Table

| Feature       | Purpose                          | Example Mechanism           |
|---------------|----------------------------------|-----------------------------|
| SSL/TLS       | Encryption                       | X.509 certs                 |
| SASL          | Authentication                   | SCRAM, PLAIN, GSSAPI        |
| ACLs          | Authorization                    | kafka-acls.sh               |
| Kerberos      | Enterprise Authentication        | GSSAPI                      |

---


# ‚úÖ 10. Kafka Monitoring and Metrics

## üîç Kafka JMX Metrics

JMX (Java Management Extensions) is the standard way Kafka exposes metrics.
You can access metrics by enabling JMX and using tools like JConsole, Prometheus JMX exporter, etc.

**Common JMX Metrics:**
- `kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec`
- `kafka.network:type=RequestMetrics,name=RequestsPerSec,request=Produce`
- `kafka.consumer:type=ConsumerFetcherManager,name=MaxLag,clientId=*`
- GC metrics from JVM (`java.lang:type=GarbageCollector,name=*`)

## üìä Tools for Monitoring

### Prometheus + Grafana
- **Prometheus JMX Exporter**: Exposes Kafka JMX metrics as Prometheus metrics.
- **Grafana Dashboards**: Visualize Kafka health, throughput, latency, etc.

**Steps:**
1. Use JMX Exporter as Java Agent in Kafka startup script.
2. Configure Prometheus to scrape metrics.
3. Import prebuilt Grafana dashboards (e.g., from Confluent or Bitnami).

### Confluent Control Center
- GUI for Kafka health, topic throughput, lag, schema usage, alerts.
- Part of Confluent Platform (requires enterprise edition for some features).

## üïí Consumer Lag Monitoring

Consumer lag = Latest offset - Committed offset

**Tools:**
- `kafka-consumer-groups.sh --describe`
- Burrow (by LinkedIn)
- Prometheus + Grafana dashboards

### Why it's important:
- Large lag means consumers are behind, potentially causing SLA issues.
- May indicate processing issues, throttled consumers, network latency.

## üíÄ Dead-letter Queues (DLQs)

Used to handle messages that failed during consumption or processing.

### Key Features:
- Prevent poison-pill messages from crashing consumers.
- DLQ is another Kafka topic.
- Implemented with Spring Kafka (or manually via Kafka APIs).

```java
@KafkaListener(topics = "input-topic", errorHandler = "myErrorHandler")
public void process(String message) {
    // handle message
}

@Bean
public ErrorHandler myErrorHandler(KafkaTemplate<?, ?> template) {
    return new SeekToCurrentErrorHandler(
        new DeadLetterPublishingRecoverer(template), 
        new FixedBackOff(1000L, 2));
}
```

## ‚úÖ Best Practices
- Monitor all critical metrics: lag, throughput, request failures, JVM GC.
- Use alerting systems based on metrics thresholds.
- Centralized logging for Kafka brokers.

---

# ‚úÖ 11. Kafka Error Handling & Retries

Error handling in Kafka applications is critical for ensuring robust and reliable data processing. Kafka provides built-in mechanisms, and frameworks like Spring Kafka extend them for more advanced use cases.

---

## üîπ Deserialization Errors

### Problem:
When a consumer receives a message that cannot be deserialized due to malformed data or wrong schema.

### Solutions:
- Use `ErrorHandlingDeserializer` from Spring Kafka.
- Log and route bad messages to a Dead Letter Topic (DLT).

```java
props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
    ErrorHandlingDeserializer.class.getName());
props.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS,
    JsonDeserializer.class.getName());
```

---

## üîπ Retry Strategies

### 1. Fixed Backoff
Retries the failed message with a constant delay.

```java
factory.setCommonErrorHandler(new DefaultErrorHandler(
    new FixedBackOff(1000L, 3))); // Retry 3 times with 1 second delay
```

### 2. Exponential Backoff
Backoff time increases exponentially with each retry.

```java
ExponentialBackOffWithMaxRetries backOff = new ExponentialBackOffWithMaxRetries(3);
backOff.setInitialInterval(1000L);
backOff.setMultiplier(2);
factory.setCommonErrorHandler(new DefaultErrorHandler(backOff));
```

---

## üîπ Dead Letter Topics (DLTs)

DLTs are special Kafka topics used to store messages that cannot be processed even after multiple retries.

### How it works:
- Consumer retries N times (using `DefaultErrorHandler`).
- After failure, message is redirected to `.DLT` topic.

```java
DefaultErrorHandler errorHandler = new DefaultErrorHandler(
    new DeadLetterPublishingRecoverer(kafkaTemplate), new FixedBackOff(1000L, 3));
```

---

## üîπ Poison Pill Messages

A poison pill is a message that consistently causes failures.

### Mitigation:
- Detect and route to DLT after N retries.
- Use message headers (like `x-retry-count`) to track retry attempts.

---

## üîç Interview Questions

### Q1: How do you handle deserialization errors in Kafka consumers?
**Answer:** Use `ErrorHandlingDeserializer` and configure a `DefaultErrorHandler` to log the error, retry, and optionally publish to a DLT.

### Q2: What's the difference between fixed backoff and exponential backoff?
**Answer:** Fixed backoff retries messages with a constant delay; exponential backoff increases the delay after each retry.

### Q3: What are dead-letter topics and when do you use them?
**Answer:** DLTs store unprocessable messages for later analysis or manual intervention. They‚Äôre used after exhausting retry attempts.

### Q4: How to identify a poison pill message?
**Answer:** Poison pills cause repeated failures and can be identified using headers or error tracking tools. Route them to DLTs to prevent retry loops.

---

## ‚úÖ Best Practices
- Always use DLTs in production.
- Configure proper retry intervals to reduce load.
- Use logging and alerting for poison pills and deserialization failures.

---

# ‚úÖ 12. Kafka Performance Tuning

Performance tuning in Kafka is essential to maximize throughput, minimize latency, and efficiently utilize system resources. Below are the key parameters and strategies to optimize Kafka producers and consumers.

---

## üî∏ Batching and Compression

### üîπ Batching
- **batch.size**:
    - Maximum number of bytes per batch for a single partition.
    - Larger batches improve throughput but may increase latency.

- **linger.ms**:
    - Time to wait before sending a batch even if it‚Äôs not full.
    - Helps in batching more records in a single request.

### üîπ Compression
- **compression.type**: `none`, `gzip`, `snappy`, `lz4`, `zstd`
    - Reduces network usage and disk I/O.
    - **snappy** and **lz4** are fast and suitable for high-throughput use cases.

---

## üî∏ Producer Tuning Parameters

| Property          | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| `acks`           | Control durability (`0`, `1`, `all`). `all` is safest, `0` is fastest.       |
| `retries`        | Number of retries on transient failures.                                    |
| `max.in.flight.requests.per.connection` | Controls ordering guarantees and parallelism. Set to 1 to ensure ordering. |
| `buffer.memory`  | Total memory available to the producer for buffering.                        |

---

## üî∏ Consumer Tuning Parameters

### üîπ Fetch Configuration
- **fetch.min.bytes**:
    - Minimum amount of data the broker should return.
    - Higher value improves throughput.

- **fetch.max.wait.ms**:
    - Max wait time if `fetch.min.bytes` is not met.
    - Useful for batching messages.

### üîπ Parallelism
- **max.poll.records**:
    - Number of records returned in one `poll()` call.
    - Tune for batch processing.

- **concurrency (Spring Kafka)**:
    - Number of concurrent consumers per listener container.
    - Set via `ConcurrentKafkaListenerContainerFactory`.

---

## üî∏ Partitioning Strategies

- **Custom Partitioner**:
    - Implement your own logic for routing messages to partitions.
    - Can help evenly distribute load or co-locate related data.

- **Key-based Partitioning**:
    - Default method; ensures order for a given key.
    - Uneven keys can lead to partition skew.

---

## ‚úÖ Best Practices

- Use **snappy** or **lz4** compression for fast throughput.
- Tune **linger.ms** and **batch.size** together for batching.
- Monitor **consumer lag** and adjust `max.poll.records` or concurrency.
- Avoid excessive batching in low-latency use cases.
- Benchmark with your actual workloads.

---

# ‚úÖ 13. Kafka High Availability & Scalability

Kafka is designed for high availability (HA) and horizontal scalability. Proper configurations and architecture decisions help ensure data durability and system resilience.

---

## üî∏ Partitioning Strategy and Balancing

### üîπ Partitioning Strategy
- Messages are distributed across **partitions** within a topic.
- Key-based partitioning ensures message ordering per key.
- Random or round-robin partitioning provides even distribution.

### üîπ Balancing
- Partitions are spread across brokers.
- Good partition design ensures even load across Kafka brokers.
- Use **Kafka Cruise Control** to auto-balance partitions.

---

## üî∏ Replication Factor and ISR (In-Sync Replicas)

### üîπ Replication Factor
- Each partition can have multiple replicas.
- One **leader**, others are **followers**.
- Recommended: 3 (minimum 2 for HA).

### üîπ ISR (In-Sync Replicas)
- Set of replicas that are fully caught up with the leader.
- Kafka only acknowledges writes to ISR members.

---

## üî∏ Avoiding Data Loss

| Setting | Description |
|--------|-------------|
| `acks=all` | Ensures data is acknowledged only after all ISR members confirm the write. |
| `min.insync.replicas` | Minimum ISR count required to accept a write. Prevents write if not enough replicas are in sync. |

> ‚úÖ Use `acks=all` **with** `min.insync.replicas >= 2` to ensure durability even during broker failure.

---

## üî∏ Multi-Cluster Architecture (MirrorMaker)

### üîπ Kafka MirrorMaker
- Tool for replicating data between Kafka clusters.
- Used for geo-redundancy, migration, and multi-datacenter setups.

### üîπ Use Cases
- **Disaster recovery**: replicate between regions.
- **Data locality**: keep consumers close to their data.
- **Cloud migration**: mirror data from on-prem to cloud.

---

## ‚úÖ Best Practices

- Always enable replication (minimum 2, ideally 3).
- Monitor ISR lag and rebalance under-replicated partitions.
- Use **Kafka Controller** monitoring to detect failovers.
- Test cluster failover scenarios proactively.
- Separate critical topics into separate partitions with dedicated replication if needed.

---

# ‚úÖ 14. Schema Registry (with Avro/Protobuf)

Kafka often works with structured data formats like **Avro** or **Protobuf**. To manage schema evolution and enforce data contracts, we use **Schema Registry**.

---

## üî∏ What is Schema Registry?

- A central service to store and retrieve schemas (data structure definitions).
- Ensures producers and consumers agree on the data structure.
- Avoids hard-coding schema information into Kafka messages.

---

## üî∏ Why Use Schema Registry?

- **Strong typing**: Avro and Protobuf provide strongly typed messages.
- **Compatibility control**: Ensure that data changes don‚Äôt break consumers.
- **Efficient serialization**: Avro and Protobuf compress data and reduce payload size.

---

## üî∏ Common Schema Formats

| Format    | Description |
|-----------|-------------|
| **Avro**  | Popular in Kafka ecosystem; supports schema evolution. |
| **Protobuf** | Compact, fast, language-neutral format from Google. |

---

## üî∏ Schema Evolution and Compatibility Rules

Schema Registry supports different compatibility modes to ensure schema changes don‚Äôt break consumers:

| Compatibility Mode | Description |
|--------------------|-------------|
| **BACKWARD**       | New schema can read data written with previous schema. |
| **FORWARD**        | Old schema can read data written with new schema. |
| **FULL**           | Both forward and backward compatible. |
| **NONE**           | No compatibility checks. |

> ‚ö†Ô∏è Always plan schema evolution carefully in production systems.

---

## üî∏ How It Works (with Avro)

1. **Producer** registers the schema with the registry.
2. **Schema Registry** returns a schema ID.
3. **Producer** sends data with the schema ID in message headers.
4. **Consumer** fetches the schema by ID from the registry to deserialize.

---

## üî∏ Code Snippet: Kafka Avro Producer Example

```java
Properties props = new Properties();
props.put("key.serializer", StringSerializer.class);
props.put("value.serializer", KafkaAvroSerializer.class);
props.put("schema.registry.url", "http://localhost:8081");

KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(props);
```

---

## üî∏ Tools

- **Confluent Schema Registry** (most widely used)
- **Karapace** (open-source alternative)
- **Apicurio** (Red Hat schema registry)

---

## ‚úÖ Best Practices

- Always version your schemas.
- Avoid schema breaking changes like deleting required fields.
- Use full compatibility mode in production for maximum safety.
- Centralize schema governance for better data contracts.

---

# ‚úÖ 15. Real-World Design Patterns in Kafka

## üîÑ Event Sourcing with Kafka
- **Concept**: Store state changes (events) rather than current state.
- **Why Kafka?**: Kafka's immutable, append-only log fits perfectly for persisting events over time.
- **How it works**:
    - Each change to the application's state is captured as an event and written to a Kafka topic.
    - Consumers rebuild current state by replaying events.

## ‚úÖ CQRS using Kafka Streams
- **CQRS (Command Query Responsibility Segregation)**: Separate read and write models.
- **Kafka Use Case**:
    - **Command Side**: Events from producers (writes).
    - **Query Side**: Kafka Streams processes events and updates materialized views (read models).
- **Tools**: `KTable`, `KStream`, local state stores.

## üß© Saga Pattern with Kafka
- **Problem**: Distributed transaction management across services.
- **Kafka Solution (Choreography)**:
    - Services communicate via events.
    - No central orchestrator ‚Äî each service reacts and emits events.
    - Example: Order Service ‚Üí emits "OrderPlaced" ‚Üí Payment Service listens ‚Üí emits "PaymentConfirmed".

## üîÅ Idempotent Consumers
- **Goal**: Prevent duplicate processing.
- **Challenges**: Kafka can redeliver messages (at-least-once by default).
- **Techniques**:
    - Use unique IDs in events.
    - Maintain a processed-event cache or DB record.
    - Ensure message processing is side-effect free or deduplicated.

## üì§ Outbox Pattern
- **Problem**: Ensuring DB and Kafka remain in sync (avoid dual writes).
- **Solution**:
    - Write DB transaction and event into an `outbox` table in the same transaction.
    - A background process (or Debezium CDC) publishes events from the outbox to Kafka.
- **Benefits**: Atomicity and consistency.

## ‚úÖ When to Use These Patterns?
| Pattern         | Use Case |
|----------------|----------|
| Event Sourcing | Auditability, replayability |
| CQRS           | Complex queries, performance isolation |
| Saga           | Long-running workflows across microservices |
| Idempotent Consumer | Preventing duplicate side effects |
| Outbox Pattern | Reliable DB-to-Kafka publishing |

## üìå Best Practices
- Ensure topic naming reflects business domains.
- Use schema registry for event compatibility.
- Monitor consumer lag and event delivery.
- Choose correct delivery semantics (at-least-once vs exactly-once).

---

# ‚úÖ 16. Testing Kafka

## üß™ Embedded Kafka for Unit/Integration Tests
- **Embedded Kafka** from `spring-kafka-test` or other test libraries allows spinning up a Kafka broker in-memory.
- **Use Cases**:
    - Unit testing producers and consumers without external dependencies.
    - Integration testing Kafka configurations.
- **Example (Spring Boot)**:
  ```java
  @SpringBootTest
  @EmbeddedKafka(partitions = 1, topics = { "test-topic" })
  public class KafkaIntegrationTest {
      @Autowired
      private KafkaTemplate<String, String> kafkaTemplate;
      
      @Test
      public void testSendReceive() {
          kafkaTemplate.send("test-topic", "key", "value");
          // assertions here
      }
  }
  ```

## üß± Kafka TestContainers
- Leverages Docker to spin up real Kafka instances for testing using the `testcontainers` library.
- **Benefits**:
    - Closer to production environment.
    - Supports custom configurations (e.g., SASL, SSL).
- **Use Case**:
  ```java
  KafkaContainer kafka = new KafkaContainer(DockerImageName.parse("confluentinc/cp-kafka:latest"));
  kafka.start();
  ```

## üß∞ Simulating Partitions, Lag, and Retries
- **Partition Testing**: Manually assign partitions and produce to simulate ordering and load balancing.
- **Lag Simulation**: Use delay in consumer polling or commit to test lag-handling logic.
- **Retry Simulation**: Throw exceptions in consumer logic and validate retry policies/backoff.

## ‚úÖ Best Practices
- Clean up brokers/topics after test runs.
- Isolate tests using random topic names.
- Assert message delivery, ordering, and headers where relevant.


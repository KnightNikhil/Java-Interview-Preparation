1. Your Spring Boot application is experiencing performance issues under high load. What are the steps you would take to identify and address the performance?
   

2. What strategies would you use to optimize the performance of a Spring Boot application?
   Let‚Äôs say my Spring Boot application is taking too long to respond to user requests. I could:
   ‚Ä¢ Implement caching for frequently accessed data.
   ‚Ä¢ Optimize database queries to reduce the load on the database.
   ‚Ä¢ Use asynchronous methods for operations like sending emails.
   ‚Ä¢ Load Balancer if traffic is high
   ‚Ä¢ Optimize the time complexity of the code
   ‚Ä¢ Use webFlux to handle a large number of concurrent connections.

3. scheduled jobs in your spring boot application is running twice in production even though you deployed only once. What might be happening?

4. So let's say your rest API allows file uploads but users complain about large files failing silently. How would you debug and fix this?

5. So uh let's say your API endpoints returns an empty JSON response even though that DB has data.

6. So in your system you have 10 spring boot microservices. Okay. Managing configuration separately has been painful. Right. Managing configurations separately has been painful. How would you handle centralized config management?

7. so how are static variables stored in memory compared to instance variable?


8. if I am using cache in my application, there will be a time cache expires, and at the same time if 100s of request tries access the data, all 100s o request will go to db?
This is an excellent and very real-world question ‚Äî and yes, what you‚Äôre describing is a classic production problem.

Short answer first, then we‚Äôll go deep.

‚∏ª

üî¥ Short Answer

Yes ‚Äî if you do nothing special, when cache expires and 100 requests arrive at the same time, all 100 can hit the DB.

This is called Cache Stampede (or Thundering Herd).

‚∏ª

1Ô∏è‚É£ What Exactly Is Happening (Timeline)

Assume:
‚Ä¢	Cache TTL = 5 minutes
‚Ä¢	DB query = expensive

Timeline

T0: Cache entry expires
T1: 100 requests arrive at same time

Each request:
1.	Checks cache ‚Üí MISS
2.	Goes to DB
3.	Fetches same data
4.	Writes back to cache

‚û°Ô∏è 100 identical DB queries
‚û°Ô∏è DB gets hammered
‚û°Ô∏è Latency spikes

‚∏ª

2Ô∏è‚É£ Why Cache Alone Does NOT Prevent This

Cache guarantees:
‚Ä¢	Fast reads on hit
‚Ä¢	Nothing on miss

Cache does not:
‚Ä¢	Coordinate concurrent misses
‚Ä¢	Serialize DB access
‚Ä¢	Deduplicate requests

‚∏ª

3Ô∏è‚É£ This Is NOT a Multithreading Bug

Important clarification:
‚Ä¢	Each request is on a different thread
‚Ä¢	Threads are doing the correct thing
‚Ä¢	Problem is coordination, not threads

‚∏ª

4Ô∏è‚É£ Common Real-World Solutions (Industry Standard)

‚úÖ Solution 1: Cache Lock / Single Flight (Most Common)

Only ONE request goes to DB.

Others wait.

Pseudocode

if (cache.get(key) == null) {
synchronized(lockForKey(key)) {
if (cache.get(key) == null) {
value = db.load(key);
cache.put(key, value);
}
}
}
return cache.get(key);

‚úî Prevents stampede
‚ùå Can block threads briefly

‚∏ª

‚úÖ Solution 2: Distributed Lock (Redis)

Used in multi-instance systems.

SETNX lock:key

Only one service instance:
‚Ä¢	Acquires lock
‚Ä¢	Loads DB
‚Ä¢	Updates cache

Others:
‚Ä¢	Spin / wait / retry

‚∏ª

‚úÖ Solution 3: Request Coalescing (Best)
‚Ä¢	First request triggers DB call
‚Ä¢	Others wait on same future
‚Ä¢	When DB returns, all get result

Used by:
‚Ä¢	Guava Cache
‚Ä¢	Caffeine
‚Ä¢	Netflix Hystrix
‚Ä¢	Resilience4j

Example (Caffeine):

cache.get(key, k -> db.load(k));

‚úî Only one DB call
‚úî No explicit locking
‚úî Highly scalable

‚∏ª

‚úÖ Solution 4: Stale-While-Revalidate (Very Popular)

Cache never fully expires.
‚Ä¢	Serve stale data
‚Ä¢	Refresh in background

User ‚Üí gets old data (fast)
Background thread ‚Üí refreshes cache

Used by:
‚Ä¢	CDNs
‚Ä¢	Payment systems
‚Ä¢	Social media

‚∏ª

‚úÖ Solution 5: TTL Jitter (Simple & Effective)

Instead of fixed TTL:

TTL = 5 min ¬± random(30 sec)

Prevents many keys expiring at once.

‚∏ª

5Ô∏è‚É£ What Payment Gateways Usually Do

Payment systems never rely only on TTL.

They combine:
‚Ä¢	Request coalescing
‚Ä¢	Short TTL
‚Ä¢	Background refresh
‚Ä¢	Idempotency keys

Because DB hits during expiry = outage risk.

‚∏ª

6Ô∏è‚É£ Important Distinction

Concept	Meaning
Cache miss	No data
Cache stampede	Many misses at same time
Synchronization	Thread-level protection
Distributed lock	Cluster-level protection


‚∏ª

7Ô∏è‚É£ Interview-Grade Answer

If asked:

‚ÄúWhen cache expires and many requests come, do they all hit DB?‚Äù

Answer:

‚ÄúYes, unless cache stampede protection is implemented. Without coordination, concurrent cache misses can cause multiple identical DB queries. This is typically solved using request coalescing, locks, or stale-while-revalidate strategies.‚Äù

‚∏ª

8Ô∏è‚É£ Final Mental Model

Cache is a book shelf.
When the book is missing:
‚Ä¢	Everyone runs to the library
‚Ä¢	Unless one person is assigned to fetch it

‚∏ª

9Ô∏è‚É£ Key Takeaway (Memorize)

Caching improves average latency, not worst-case concurrency.
Stampede protection is mandatory at scale.

‚∏ª

If you want next, I can:
‚Ä¢	Show Java code with CompletableFuture
‚Ä¢	Explain how Caffeine solves this internally
‚Ä¢	Explain Redis-based locking
‚Ä¢	Connect this to YouTube view count problem

Just tell me where to go next.